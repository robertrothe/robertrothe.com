<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robertrothe - Lokale Speech-to-Text</title>
    <style>
        body { font-family: monospace; margin: 0; padding: 0; background-color: #000; color: #0f0; overflow-x: hidden; }
        #main-container { padding: 40px 20px; background-color: #000; color: #0f0; min-height: 100vh; display: flex; flex-direction: column; align-items: center; justify-content: center; text-align: center; }
        #transcript { width: 80%; max-width: 600px; min-height: 200px; padding: 10px; border: 1px solid #0f0; background-color: #111; border-radius: 5px; overflow-y: auto; font-size: 16px; line-height: 1.5; margin-bottom: 20px; text-align: left; }
        #status { margin: 20px; color: #ff0; font-size: 1.1em; }
        .ascii-button { margin: 5px; padding: 10px 20px; font-size: 14px; color: #0f0; background-color: #000; border: 1px solid #0f0; border-radius: 5px; cursor: pointer; transition: background-color 0.3s ease, color 0.3s ease; }
        .ascii-button:hover:not(:disabled) { background-color: #0f0; color: #000; }
        .ascii-button:disabled { border-color: #444; color: #666; cursor: not-allowed; }
    </style>
</head>
<body>
    <main id="main-container">
        <h1>Lokale Speech-to-Text Demo</h1>
        <p>(Powered by Whisper Tiny - 100% lokal in Ihrem Browser)</p>
        <div id="status">Bereit. Klicke Start, um das KI-Modell zu laden.</div>
        <div id="transcript">
            <p>Transkription erscheint hier...</p>
        </div>
        <button class="ascii-button" id="startBtn">[ Aufnahme starten ]</button>
        <button class="ascii-button" id="stopBtn" disabled>[ Aufnahme stoppen ]</button>
        <button class="ascii-button" id="downloadBtn" disabled>[ Transkript herunterladen ]</button>
    </main>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';
        
        env.remoteHost = 'https://huggingface.co';
        env.remotePathTemplate = '{model}/resolve/{revision}/';
        env.allowLocalModels = false;

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const downloadBtn = document.getElementById('downloadBtn');
        const transcriptDiv = document.getElementById('transcript');
        const statusDiv = document.getElementById('status');

        let transcriber = null;
        let isRecording = false;
        let fullTranscript = "";
        let audioContext;
        let stream;
        let audioWorkletNode;

        const workletCode = `
            class AudioProcessor extends AudioWorkletProcessor {
                constructor() {
                    super();
                }
                process(inputs, outputs, parameters) {
                    const input = inputs[0];
                    if (input.length > 0) {
                        const pcmData = input[0];
                        this.port.postMessage(pcmData);
                    }
                    return true;
                }
            }
            registerProcessor('audio-processor', AudioProcessor);
        `;

        async function initializeModel() {
            if (transcriber) return;
            statusDiv.textContent = 'Lade KI-Modell...';
            startBtn.disabled = true;
            try {
                transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny');
                statusDiv.textContent = 'Modell geladen. Bereit für die Aufnahme.';
            } catch (error) {
                console.error("Fehler beim Laden des Modells:", error);
                statusDiv.textContent = 'Fehler: Das KI-Modell konnte nicht geladen werden.';
            } finally {
                if(transcriber && !isRecording) startBtn.disabled = false;
            }
        }

        async function startRecording() {
            if (!transcriber) {
                await initializeModel();
                if (!transcriber) return;
            }
            if (isRecording) return;

            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const source = audioContext.createMediaStreamSource(stream);

                const blob = new Blob([workletCode], { type: 'application/javascript' });
                const workletURL = URL.createObjectURL(blob);
                await audioContext.audioWorklet.addModule(workletURL);
                
                audioWorkletNode = new AudioWorkletNode(audioContext, 'audio-processor');
                source.connect(audioWorkletNode);

                let audioBuffer = [];
                const bufferDurationSeconds = 5;
                const bufferSize = audioContext.sampleRate * bufferDurationSeconds;

                audioWorkletNode.port.onmessage = async (event) => {
                    audioBuffer.push(...event.data);
                    if (audioBuffer.length >= bufferSize) {
                        const processData = new Float32Array(audioBuffer.splice(0, bufferSize));
                        statusDiv.textContent = 'Verarbeite Audio...';
                        
                        const output = await transcriber(processData, {
                            language: 'german',
                            task: 'transcribe',
                        });

                        const newText = output.text;
                        if (newText && newText.trim() !== "") {
                            fullTranscript += newText + " ";
                            transcriptDiv.innerHTML = `<p>${fullTranscript}</p>`;
                            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
                        }
                        
                        if (isRecording) {
                           statusDiv.textContent = 'Aufnahme läuft...';
                        }
                    }
                };

                isRecording = true;
                startBtn.disabled = true;
                stopBtn.disabled = false;
                downloadBtn.disabled = true;
                transcriptDiv.innerHTML = "<p>Aufnahme gestartet...</p>";
                fullTranscript = "";
                statusDiv.textContent = 'Aufnahme läuft...';

            } catch (error) {
                console.error("Fehler beim Start der Aufnahme:", error);
                statusDiv.textContent = 'Fehler: Mikrofonzugriff verweigert oder nicht möglich.';
            }
        }

        function stopRecording() {
            if (isRecording) {
                stream.getTracks().forEach(track => track.stop());
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close();
                }
                isRecording = false;
                startBtn.disabled = false;
                stopBtn.disabled = true;
                downloadBtn.disabled = fullTranscript.trim().length === 0;
                statusDiv.textContent = 'Aufnahme gestoppt. Modell ist bereit.';
            }
        }
        
        function downloadTranscript() {
            const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
            const filename = `lokales-transkript-${timestamp}.txt`;
            const blob = new Blob([fullTranscript.trim()], { type: "text/plain;charset=UTF-8" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.href = url;
            a.download = filename;
            a.click();
            URL.revokeObjectURL(url);
        }

        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
        downloadBtn.addEventListener('click', downloadTranscript);

    </script>
</body>
</html>
