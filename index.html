<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robertrothe - Lokale Speech-to-Text</title>
    <!-- 
        WICHTIG: Der alte script-Tag hier wird ENTFERNT.
        Der Import passiert jetzt direkt im Modul-Skript am Ende der Seite.
    -->
    <style>
        body { font-family: monospace; margin: 0; padding: 0; background-color: #000; color: #0f0; overflow-x: hidden; }
        #main-container { padding: 40px 20px; background-color: #000; color: #0f0; min-height: 100vh; display: flex; flex-direction: column; align-items: center; justify-content: center; text-align: center; }
        #transcript { width: 80%; max-width: 600px; min-height: 200px; padding: 10px; border: 1px solid #0f0; background-color: #111; border-radius: 5px; overflow-y: auto; font-size: 16px; line-height: 1.5; margin-bottom: 20px; text-align: left; }
        #status { margin: 20px; color: #ff0; font-size: 1.1em; }
        .ascii-button { margin: 5px; padding: 10px 20px; font-size: 14px; color: #0f0; background-color: #000; border: 1px solid #0f0; border-radius: 5px; cursor: pointer; transition: background-color 0.3s ease, color 0.3s ease; }
        .ascii-button:hover:not(:disabled) { background-color: #0f0; color: #000; }
        .ascii-button:disabled { border-color: #444; color: #666; cursor: not-allowed; }
    </style>
</head>
<body>

    <main id="main-container">
        <h1>Lokale Speech-to-Text Demo</h1>
        <p>(Powered by OpenAI's Whisper - 100% lokal in Ihrem Browser)</p>
        
        <div id="status">Lade KI-Modell... Bitte haben Sie einen Moment Geduld.</div>

        <div id="transcript">
            <p>Transkription erscheint hier...</p>
        </div>
        
        <button class="ascii-button" id="startBtn" disabled>[ Aufnahme starten ]</button>
        <button class="ascii-button" id="stopBtn" disabled>[ Aufnahme stoppen ]</button>
        <button class="ascii-button" id="downloadBtn" disabled>[ Transkript herunterladen ]</button>
    </main>

    <!-- HIER IST DIE ÄNDERUNG: type="module" -->
    <script type="module">
        // UND HIER: Die Bibliothek wird direkt per import geladen.
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const downloadBtn = document.getElementById('downloadBtn');
        const transcriptDiv = document.getElementById('transcript');
        const statusDiv = document.getElementById('status');

        let transcriber = null;
        let mediaRecorder = null;
        let isRecording = false;
        let fullTranscript = "";
        
        async function initializeModel() {
            statusDiv.textContent = 'Lade KI-Modell... (Dies kann beim ersten Mal dauern)';
            try {
                // Diese Zeile funktioniert jetzt, weil 'pipeline' korrekt importiert wurde.
                transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny', {
                    multilingual: false,
                });
                
                statusDiv.textContent = 'Modell geladen. Bereit für die Aufnahme.';
                startBtn.disabled = false;
            } catch (error) {
                console.error("Fehler beim Laden des Modells:", error);
                statusDiv.textContent = 'Fehler: Das KI-Modell konnte nicht geladen werden.';
            }
        }

        async function startRecording() {
            if (!transcriber) {
                statusDiv.textContent = 'Fehler: Das Modell ist noch nicht bereit.';
                return;
            }
            if (isRecording) return;

            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.addEventListener('dataavailable', async (event) => {
                    statusDiv.textContent = 'Verarbeite Audio...';
                    
                    const audioBlob = new Blob([event.data], { type: event.data.type });
                    const audioBuffer = await audioBlob.arrayBuffer();
                    const audioData = await convertAudioBufferToFloat32(audioBuffer);

                    const output = await transcriber(audioData, {
                        language: 'german',
                        task: 'transcribe',
                    });

                    const newText = output.text;
                    if (newText && newText.trim() !== "") {
                        fullTranscript += newText + " ";
                        transcriptDiv.innerHTML = `<p>${fullTranscript}</p>`;
                        transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
                    }
                    
                    if (isRecording) {
                       statusDiv.textContent = 'Aufnahme läuft...';
                    }
                });

                mediaRecorder.start(5000); 

                isRecording = true;
                startBtn.disabled = true;
                stopBtn.disabled = false;
                downloadBtn.disabled = true;
                transcriptDiv.innerHTML = "<p>Aufnahme gestartet...</p>";
                fullTranscript = "";
                statusDiv.textContent = 'Aufnahme läuft...';

            } catch (error) {
                console.error("Fehler beim Start der Aufnahme:", error);
                statusDiv.textContent = 'Fehler: Mikrofonzugriff verweigert oder nicht möglich.';
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            isRecording = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            downloadBtn.disabled = fullTranscript.trim().length === 0;
            statusDiv.textContent = 'Aufnahme gestoppt. Modell ist bereit.';
        }
        
        async function convertAudioBufferToFloat32(audioBuffer) {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const decodedData = await audioContext.decodeAudioData(audioBuffer);
            return decodedData.getChannelData(0);
        }

        function downloadTranscript() {
            const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
            const filename = `lokales-transkript-${timestamp}.txt`;
            const blob = new Blob([fullTranscript.trim()], { type: "text/plain;charset=UTF-8" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.href = url;
            a.download = filename;
            a.click();
            URL.revokeObjectURL(url);
        }

        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
        downloadBtn.addEventListener('click', downloadTranscript);

        initializeModel();

    </script>
</body>
</html>
